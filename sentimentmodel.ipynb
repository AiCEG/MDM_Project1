{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:997)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   text stars  sentiment\n",
      "4     Visited this restaurant recently and was impre...     2        0.0\n",
      "11    Unfortunately our experience was not positive....     2        0.0\n",
      "17    Freundliche Bedienung, allerdings waren die Po...     3        0.0\n",
      "21    The food and ambience was overall good. The ma...     3        0.0\n",
      "23    Wir haben heute Mittag für ca. Chf 70.00 beste...     3        0.0\n",
      "...                                                 ...   ...        ...\n",
      "1856              Gute Küche - Bedienung etwas eigen...     3        0.0\n",
      "1888                       Gutes Essen und faire Preise     3        0.0\n",
      "1891                      Rezesion schon früher gemacht     3        0.0\n",
      "1900                                               Nice     3        0.0\n",
      "1923  Decent restaurant. Nothing special. Unrefined ...     3        0.0\n",
      "\n",
      "[247 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "#read reviews.json\n",
    "df = pd.read_json('reviews.json')\n",
    "#remove rows with missing values\n",
    "\n",
    "\n",
    "#remove stars text from stars column\n",
    "df['stars'] = df['stars'].str.replace('stars', '')\n",
    "df['stars'] = df['stars'].str.strip()\n",
    "\n",
    "\n",
    "#map stars to sentiment\n",
    "df['sentiment'] = df['stars'].map({'1':0, '2':0, '3':0, '4':1, '5':1})\n",
    "\n",
    "#remove sentiment with Nan\n",
    "df = df.dropna(subset=['sentiment'])\n",
    "\n",
    "#show sentiment 0\n",
    "print(df[df['sentiment'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 0.8764044943820225\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['sentiment'], test_size=0.25, random_state=42)\n",
    "\n",
    "# Converting text into vectors\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words('english'), ngram_range=(1, 2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "model = MultinomialNB(alpha=0.1)\n",
    "model.fit(X_train_vec, y_train)\n",
    "\n",
    "predictions = model.predict(X_test_vec)\n",
    "print(\"Accuracy=\", accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "new_reviews = [ 'It was really bad', 'was really good', 'It was fine', 'It really was worse than expected', 'I hate this place']\n",
    "new_reviews_vec = vectorizer.transform(new_reviews)\n",
    "predictions = model.predict(new_reviews_vec)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5694 unique tokens.\n",
      "Epoch 1/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.8334 - loss: 0.4637 - val_accuracy: 0.8933 - val_loss: 0.2893 - learning_rate: 0.0100\n",
      "Epoch 2/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9139 - loss: 0.1890 - val_accuracy: 0.8904 - val_loss: 0.3540 - learning_rate: 0.0100\n",
      "Epoch 3/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9771 - loss: 0.0657 - val_accuracy: 0.8736 - val_loss: 0.5036 - learning_rate: 0.0100\n",
      "Epoch 4/20\n",
      "\u001b[1m45/45\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9849 - loss: 0.0343 - val_accuracy: 0.8427 - val_loss: 0.5564 - learning_rate: 0.0100\n",
      "Epoch 4: early stopping\n",
      "Accuracy: 0.8426966071128845\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Conv1D, MaxPooling1D, GlobalMaxPooling1D,BatchNormalization, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it's already preprocessed\n",
    "texts = df['text'].values\n",
    "labels = df['sentiment'].values\n",
    "\n",
    "# Tokenizing text\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "print(f'Found {len(word_index)} unique tokens.')\n",
    "\n",
    "# Padding sequences to ensure uniform input size\n",
    "data = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "vocab_size = 10000  # Example vocabulary size\n",
    "embedding_dim = 128  # Dimensionality of the embedding layer\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size, output_dim=embedding_dim),\n",
    "    Conv1D(64, 5, activation='relu'),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # For binary classification\n",
    "])\n",
    "\n",
    "# Compilation of the model\n",
    "optimizer = Adam(learning_rate=0.01)\n",
    "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=0.01, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test), callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Accuracy: {scores[1]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "Review: I absolutely loved the food and the service was great!\n",
      "Predicted Sentiment: Positive 0.999999463558197\n",
      "Review: Worst experience ever. Will not be coming back.\n",
      "Predicted Sentiment: Negative 0.0030046089086681604\n",
      "Review: It was okay, nothing special.\n",
      "Predicted Sentiment: Negative 0.04271911829710007\n",
      "Review: The ambiance was wonderful but the food was only average.\n",
      "Predicted Sentiment: Negative 0.32472190260887146\n",
      "Review: Disappointed with the late delivery.\n",
      "Predicted Sentiment: Negative 0.4383382201194763\n"
     ]
    }
   ],
   "source": [
    "# Sample new reviews\n",
    "new_reviews = [\n",
    "    \"I absolutely loved the food and the service was great!\",\n",
    "    \"Worst experience ever. Will not be coming back.\",\n",
    "    \"It was okay, nothing special.\",\n",
    "    \"The ambiance was wonderful but the food was only average.\",\n",
    "    \"Disappointed with the late delivery.\"\n",
    "]\n",
    "\n",
    "# Convert the reviews to sequences\n",
    "sequences = tokenizer.texts_to_sequences(new_reviews)\n",
    "\n",
    "# Pad the sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=100)\n",
    "\n",
    "# Predicting sentiment\n",
    "predictions = model.predict(padded_sequences)\n",
    "\n",
    "# Interpreting the predictions\n",
    "interpreted_predictions = [f\"Positive {pred}\" if pred > 0.6 else f\"Negative {pred}\" for pred in predictions.flatten()]\n",
    "\n",
    "# Printing the results\n",
    "for review, sentiment in zip(new_reviews, interpreted_predictions):\n",
    "    print(f\"Review: {review}\\nPredicted Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.models.save_model(model, 'sentiment_analysis_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('sentiment_analysis_model.keras')\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "\n",
    "loaded_model = load_model('sentiment_analysis_model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class MyCustomLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, my_custom_param=32, **kwargs):\n",
    "        super(MyCustomLayer, self).__init__(**kwargs)\n",
    "        self.my_custom_param = my_custom_param\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Example variable based on custom parameter\n",
    "        self.kernel = self.add_weight(\"kernel\",\n",
    "                                      shape=[int(input_shape[-1]),\n",
    "                                             self.my_custom_param])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.kernel)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(MyCustomLayer, self).get_config()\n",
    "        config.update({\"my_custom_param\": self.my_custom_param})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
